* 




* Fixed by adding params "default_auto_reply" in autogen.UserProxyAgent
default_auto_reply="Sorry, I couldn't process that request.",  # Set a default auto-reply message here, refer:https://github.com/microsoft/autogen/issues/279#issuecomment-1783656203

#===============================get error==========================
(pyautogen) root@1cfeec487615:/home/tmp/mingwork/llm_sample# python autogen/multiagents_groupchat/multiagents_groupchat.py
user_proxy (to chat_manager):

Query the latest news on yahoo.com,randomly pick one about TESLA,scrape the article content,and form a post for writing a blog

--------------------------------------------------------------------------------
Coder (to chat_manager):

 #!/bin/bash
# Get latest news from Yahoo.com
curl -o news.html https://www.yahoo.com/news

# Pick a random article about Tesla
grep -E 'Tesla' news.html | cut -d'<' -f2 | head -n1 | tr -d '\'' > tesla_article.html

# Scrape the article content
python scraper.py tesla_article.html > tesla_blog.txt

# Post for writing a blog
echo "Here is a post about the latest news on Tesla:" >> blog.txt
cat tesla_blog.txt >> blog.txt

TERMINATE

--------------------------------------------------------------------------------
Product_manager (to chat_manager):

 The script you provided is written in Bash and uses various commands to scrape the latest news article about Tesla from Yahoo.com, extract its content, and write a post for a blog. Here's how it works:

1. First, the script retrieves the latest news from Yahoo.com using the `curl` command and saves it as an HTML file called "news.html".
2. Next, the script searches for the first instance of the word "Tesla" in the news.html file using the `grep` command, extracts the content of that article using `cut`, removes any extra characters with `tr`, and saves it as a new HTML file called "tesla\_article.html".
3. After that, the script uses Python to scrape the content of the tesla\_article.html file using a Python script called "scraper.py" and saves it as a text file called "tesla\_blog.txt".
4. Finally, the script writes a post for a blog by adding a heading and appending the contents of the tesla\_blog.txt file to a new text file called "blog.txt".

This script can be useful if you want to keep track of the latest news about Tesla or other topics on Yahoo.com, and automatically generate blog posts based on that news. However, you'll need to modify the script to fit your specific needs, such as changing the Python scraper script to extract different types of information from the news articles.

--------------------------------------------------------------------------------
user_proxy (to chat_manager):



--------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/tmp/mingwork/llm_sample/autogen/multiagents_groupchat/multiagents_groupchat.py", line 79, in <module>
    run_agentchat_multiagents_groupchat()
  File "/home/tmp/mingwork/llm_sample/autogen/multiagents_groupchat/multiagents_groupchat.py", line 67, in run_agentchat_multiagents_groupchat
    user_proxy.initiate_chat(
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py", line 531, in initiate_chat
    self.send(self.generate_init_message(**context), recipient, silent=silent)
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py", line 334, in send
    recipient.receive(message, self, request_reply, silent)
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py", line 462, in receive
    reply = self.generate_reply(messages=self.chat_messages[sender], sender=sender)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py", line 781, in generate_reply
    final, reply = reply_func(self, messages=messages, sender=sender, config=reply_func_tuple["config"])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/autogen/agentchat/groupchat.py", line 162, in run_chat
    speaker = groupchat.select_speaker(speaker, self)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/autogen/agentchat/groupchat.py", line 91, in select_speaker
    final, name = selector.generate_oai_reply(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py", line 606, in generate_oai_reply
    response = oai.ChatCompletion.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/autogen/oai/completion.py", line 834, in create
    return cls._get_response(params, raise_on_ratelimit_or_timeout=raise_on_ratelimit_or_timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/autogen/oai/completion.py", line 224, in _get_response
    response = openai_completion.create(request_timeout=request_timeout, **config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/openai/api_resources/chat_completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/openai/api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/openai/api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/openai/api_requestor.py", line 765, in _interpret_response_line
    raise self.handle_error_response(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/openai/api_requestor.py", line 418, in handle_error_response
    error_code=error_data.get("code"),
               ^^^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'get'
(pyautogen) root@1cfeec487615:/home/tmp/mingwork/llm_sample# 













* got error 
*** Please override the openai based on pip install openai==0.28  pyautogen==0.1.14 
#openai.api_key = api_key  # supply your API key however you choose
#openai.api_base= base_url # supply your api base URL If you have your own LLM
#===============================get error==========================
(pyautogen) root@1cfeec487615:/home/tmp/mingwork/llm_sample# python autogen/multiagents_groupchat/multiagents_groupchat.py

user_proxy (to chat_manager):

Query the latest news on yahoo.com,randomly pick one about TESLA,scrape the article content,and form a post for writing a blog

--------------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/tmp/mingwork/llm_sample/autogen/multiagents_groupchat/multiagents_groupchat.py", line 62, in <module>
    run_agentchat_multiagents_groupchat()
  File "/home/tmp/mingwork/llm_sample/autogen/multiagents_groupchat/multiagents_groupchat.py", line 50, in run_agentchat_multiagents_groupchat
    user_proxy.initiate_chat(
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py", line 531, in initiate_chat

    self.send(self.generate_init_message(**context), recipient, silent=silent)
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py", line 334, in send
    recipient.receive(message, self, request_reply, silent)
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py", line 462, in receive
    reply = self.generate_reply(messages=self.chat_messages[sender], sender=sender)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py", line 781, in generate_reply
    final, reply = reply_func(self, messages=messages, sender=sender, config=reply_func_tuple["config"])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/autogen/agentchat/groupchat.py", line 162, in run_chat
    speaker = groupchat.select_speaker(speaker, self)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/autogen/agentchat/groupchat.py", line 91, in select_speaker
    final, name = selector.generate_oai_reply(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/autogen/agentchat/conversable_agent.py", line 606, in generate_oai_reply
    response = oai.ChatCompletion.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/autogen/oai/completion.py", line 834, in create
    return cls._get_response(params, raise_on_ratelimit_or_timeout=raise_on_ratelimit_or_timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/autogen/oai/completion.py", line 224, in _get_response
    response = openai_completion.create(request_timeout=request_timeout, **config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/openai/api_resources/chat_completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 149, in create
    ) = cls.__prepare_create_request(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 106, in __prepare_create_request
    requestor = api_requestor.APIRequestor(
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/openai/api_requestor.py", line 138, in __init__
    self.api_key = key or util.default_api_key()
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tmp/mingwork/llm_sample/pyautogen/lib/python3.12/site-packages/openai/util.py", line 186, in default_api_key
    raise openai.error.AuthenticationError(
openai.error.AuthenticationError: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.




========

